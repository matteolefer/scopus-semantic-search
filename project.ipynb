{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f534806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Target device for AI: GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = 'scopus_full_data_v2.csv'  # Change this to your actual file path\n",
    "OUTPUT_DIR = 'output/'\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2' # Excellent balance of speed/performance for semantic search\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Setup complete. Target device for AI: {'GPU' if pd.Series([1]).dtype == 'int' else 'CPU'}\")\n",
    "# Note: sentence_transformers automatically detects CUDA (GPU) if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e521ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully. Shape: (19805, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>chapter_title</th>\n",
       "      <th>doi</th>\n",
       "      <th>scopus_id</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>cover_date</th>\n",
       "      <th>book_title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>aggregation_type</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>abstract</th>\n",
       "      <th>description</th>\n",
       "      <th>author_keywords</th>\n",
       "      <th>ASJC</th>\n",
       "      <th>ASJC_translation</th>\n",
       "      <th>reference_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201800000</td>\n",
       "      <td>Public health and international epidemiology for radiology</td>\n",
       "      <td>10.1007/978-3-319-98485-8_15</td>\n",
       "      <td>85077976956</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Radiology in Global Health: Strategies, Implementation, and Applications</td>\n",
       "      <td>Springer International Publishing</td>\n",
       "      <td>Book</td>\n",
       "      <td>Pongpirul K.; Lungren M.P.</td>\n",
       "      <td>Department of Radiology, Stanford University School of Medicine; Bumrungrad International Hospit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2700</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201800001</td>\n",
       "      <td>Flexible Printed Active Antenna for Digital Television Reception</td>\n",
       "      <td>10.23919/PIERS.2018.8597669</td>\n",
       "      <td>85060936020</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Progress in Electromagnetics Research Symposium</td>\n",
       "      <td>Institute of Electrical and Electronics Engineers Inc.</td>\n",
       "      <td>Conference Proceeding</td>\n",
       "      <td>Pratumsiri T.; Janpugdee P.</td>\n",
       "      <td>Department of Electrical Engineering, Wireless Network and Future Internet Research Unit, Chulal...</td>\n",
       "      <td>¬© 2018 The Institute of Electronics, Information and Communication Engineers (IEICE).This paper ...</td>\n",
       "      <td>This paper presents the development of a flexible printed active antenna for the digital televis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'$': '2208'}, {'$': '2504'}]</td>\n",
       "      <td>Electrical and Electronic Engineering, Materials Chemistry</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201800002</td>\n",
       "      <td>Parametric study of hydrogen production via sorption enhanced steam methane reforming in a circu...</td>\n",
       "      <td>10.1016/j.ces.2018.08.042</td>\n",
       "      <td>85052201238</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Chemical Engineering Science</td>\n",
       "      <td>Elsevier Ltd</td>\n",
       "      <td>Journal</td>\n",
       "      <td>Phuakpunk K.; Assabumrungrat S.; Chalermsinsuwan B.; Putivisutisak S.</td>\n",
       "      <td>Fuels Research Center, Department of Chemical Technology, Faculty of Science, Chulalongkorn Univ...</td>\n",
       "      <td>¬© 2018 Elsevier LtdComputational fluid dynamics was applied for sorption enhanced steam methane ...</td>\n",
       "      <td>Computational fluid dynamics was applied for sorption enhanced steam methane reforming (SESMR) o...</td>\n",
       "      <td>Circulating fluidized bed; Computational fluid dynamics; Multiphase flow models; Riser; Sorption...</td>\n",
       "      <td>[{'$': '1600'}, {'$': '1500'}, {'$': '2209'}]</td>\n",
       "      <td>Chemistry, Chemical Engineering, Industrial and Manufacturing Engineering</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  \\\n",
       "0  201800000   \n",
       "1  201800001   \n",
       "2  201800002   \n",
       "\n",
       "                                                                                         chapter_title  \\\n",
       "0                                           Public health and international epidemiology for radiology   \n",
       "1                                     Flexible Printed Active Antenna for Digital Television Reception   \n",
       "2  Parametric study of hydrogen production via sorption enhanced steam methane reforming in a circu...   \n",
       "\n",
       "                            doi    scopus_id  publication_year  cover_date  \\\n",
       "0  10.1007/978-3-319-98485-8_15  85077976956              2018  2018-12-31   \n",
       "1   10.23919/PIERS.2018.8597669  85060936020              2018  2018-12-31   \n",
       "2     10.1016/j.ces.2018.08.042  85052201238              2018  2018-12-31   \n",
       "\n",
       "                                                                 book_title  \\\n",
       "0  Radiology in Global Health: Strategies, Implementation, and Applications   \n",
       "1                           Progress in Electromagnetics Research Symposium   \n",
       "2                                              Chemical Engineering Science   \n",
       "\n",
       "                                                publisher  \\\n",
       "0                       Springer International Publishing   \n",
       "1  Institute of Electrical and Electronics Engineers Inc.   \n",
       "2                                            Elsevier Ltd   \n",
       "\n",
       "        aggregation_type  \\\n",
       "0                   Book   \n",
       "1  Conference Proceeding   \n",
       "2                Journal   \n",
       "\n",
       "                                                                 authors  \\\n",
       "0                                             Pongpirul K.; Lungren M.P.   \n",
       "1                                            Pratumsiri T.; Janpugdee P.   \n",
       "2  Phuakpunk K.; Assabumrungrat S.; Chalermsinsuwan B.; Putivisutisak S.   \n",
       "\n",
       "                                                                                           affiliation  \\\n",
       "0  Department of Radiology, Stanford University School of Medicine; Bumrungrad International Hospit...   \n",
       "1  Department of Electrical Engineering, Wireless Network and Future Internet Research Unit, Chulal...   \n",
       "2  Fuels Research Center, Department of Chemical Technology, Faculty of Science, Chulalongkorn Univ...   \n",
       "\n",
       "                                                                                              abstract  \\\n",
       "0                                                                                                  NaN   \n",
       "1  ¬© 2018 The Institute of Electronics, Information and Communication Engineers (IEICE).This paper ...   \n",
       "2  ¬© 2018 Elsevier LtdComputational fluid dynamics was applied for sorption enhanced steam methane ...   \n",
       "\n",
       "                                                                                           description  \\\n",
       "0                                                                                                  NaN   \n",
       "1  This paper presents the development of a flexible printed active antenna for the digital televis...   \n",
       "2  Computational fluid dynamics was applied for sorption enhanced steam methane reforming (SESMR) o...   \n",
       "\n",
       "                                                                                       author_keywords  \\\n",
       "0                                                                                                  NaN   \n",
       "1                                                                                                  NaN   \n",
       "2  Circulating fluidized bed; Computational fluid dynamics; Multiphase flow models; Riser; Sorption...   \n",
       "\n",
       "                                            ASJC  \\\n",
       "0                                           2700   \n",
       "1                 [{'$': '2208'}, {'$': '2504'}]   \n",
       "2  [{'$': '1600'}, {'$': '1500'}, {'$': '2209'}]   \n",
       "\n",
       "                                                            ASJC_translation  \\\n",
       "0                                                                   Medicine   \n",
       "1                 Electrical and Electronic Engineering, Materials Chemistry   \n",
       "2  Chemistry, Chemical Engineering, Industrial and Manufacturing Engineering   \n",
       "\n",
       "   reference_count  \n",
       "0               76  \n",
       "1                4  \n",
       "2               42  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE, on_bad_lines='skip')\n",
    "    print(f\"‚úÖ Data loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found. Please check the INPUT_FILE path.\")\n",
    "\n",
    "# Display structure\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c248736",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "975c9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values ---\n",
      "file_name              0\n",
      "chapter_title          1\n",
      "doi                 1135\n",
      "scopus_id              0\n",
      "publication_year       0\n",
      "cover_date             0\n",
      "book_title             0\n",
      "publisher              5\n",
      "aggregation_type       0\n",
      "authors                0\n",
      "affiliation           12\n",
      "abstract             529\n",
      "description          529\n",
      "author_keywords     3467\n",
      "ASJC                   0\n",
      "ASJC_translation       0\n",
      "reference_count        0\n",
      "dtype: int64\n",
      "\n",
      "--- Column Types ---\n",
      "file_name            int64\n",
      "chapter_title       object\n",
      "doi                 object\n",
      "scopus_id            int64\n",
      "publication_year     int64\n",
      "cover_date          object\n",
      "book_title          object\n",
      "publisher           object\n",
      "aggregation_type    object\n",
      "authors             object\n",
      "affiliation         object\n",
      "abstract            object\n",
      "description         object\n",
      "author_keywords     object\n",
      "ASJC                object\n",
      "ASJC_translation    object\n",
      "reference_count      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Inspect columns and missing values to plan our cleaning strategy\n",
    "print(\"--- Missing Values ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Column Types ---\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75d247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning abstracts and metadata...\n",
      "üìâ Initial Row Count: 19805\n",
      "‚úÖ Final Row Count (Strict Cleaning): 19276\n",
      "üóëÔ∏è Dropped 529 rows due to missing/short descriptions.\n"
     ]
    }
   ],
   "source": [
    "def clean_abstract(text):\n",
    "    \"\"\"\n",
    "    Removes copyright headers and common artifacts from Scopus abstracts.\n",
    "    Example: '¬© 2018 IEEE. This paper presents...' -> 'This paper presents...'\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Pattern 1: Remove ¬© [Year] [Publisher]. (e.g., ¬© 2018 Elsevier B.V.)\n",
    "    # We look for the copyright symbol, followed by chars, until a period that isn't a decimal.\n",
    "    text = re.sub(r'^¬© \\d{4}.*?\\.(?=\\s*[A-Z])', '', text)\n",
    "    \n",
    "    # Pattern 2: Remove explicit \"All rights reserved.\"\n",
    "    text = re.sub(r'(?i)all rights reserved\\.?', '', text)\n",
    "    \n",
    "    # Normalize whitespace (replace multiple spaces/newlines with single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_authors(authors):\n",
    "    \"\"\"Clean author lists to be readable lists.\"\"\"\n",
    "    if pd.isna(authors):\n",
    "        return \"Unknown\"\n",
    "    return str(authors).replace(';', ',')\n",
    "\n",
    "# --- Apply Cleaning ---\n",
    "\n",
    "# 1. Create a stable ID (prefer Scopus ID, fallback to DOI or Index)\n",
    "df['doc_id'] = df['scopus_id'].fillna(df['doi']).fillna(df.index.to_series().astype(str))\n",
    "\n",
    "# 2. Clean Text Fields\n",
    "print(\"üßπ Cleaning abstracts and metadata...\")\n",
    "# ... (Previous cleaning functions remain the same) ...\n",
    "\n",
    "# 1. Inspect Abstract vs Description\n",
    "# Scopus sometimes puts the text in 'abstract' and sometimes in 'description'.\n",
    "# We should coalesce them: take abstract; if missing, take description.\n",
    "df['final_abstract'] = df['abstract'].fillna(df['description']).fillna('')\n",
    "\n",
    "# 2. Apply cleaning to this new combined field\n",
    "df['clean_abstract'] = df['final_abstract'].apply(clean_abstract)\n",
    "df['clean_title'] = df['chapter_title'].fillna(\"Untitled Document\")\n",
    "\n",
    "# --- STRICT FILTERING ---\n",
    "# We calculate the length of the cleaned abstract\n",
    "df['abstract_len'] = df['clean_abstract'].str.len()\n",
    "\n",
    "# Define a threshold. \n",
    "# If an abstract is less than 50 characters, it's likely \"No abstract available\" or garbage text.\n",
    "MIN_ABSTRACT_LENGTH = 50 \n",
    "\n",
    "print(f\"üìâ Initial Row Count: {len(df)}\")\n",
    "\n",
    "# Filter: Keep only rows where Abstract is long enough\n",
    "df_clean = df[df['abstract_len'] > MIN_ABSTRACT_LENGTH].copy()\n",
    "\n",
    "print(f\"‚úÖ Final Row Count (Strict Cleaning): {len(df_clean)}\")\n",
    "print(f\"üóëÔ∏è Dropped {len(df) - len(df_clean)} rows due to missing/short descriptions.\")\n",
    "\n",
    "# Update the main dataframe variable\n",
    "df = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a060b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Combined text field created. Average length: 1619 characters.\n"
     ]
    }
   ],
   "source": [
    "def create_combined_text(row):\n",
    "    # Extract keywords, handling NaNs\n",
    "    keywords = str(row['author_keywords']) if not pd.isna(row['author_keywords']) else \"\"\n",
    "    \n",
    "    # Construct the semantic blob\n",
    "    # We put the title twice explicitly if the abstract is missing to give it weight, \n",
    "    # but standard practice is just Title + Abstract.\n",
    "    text_blob = f\"Title: {row['clean_title']}. Abstract: {row['clean_abstract']}\"\n",
    "    \n",
    "    if keywords:\n",
    "        text_blob += f\" Keywords: {keywords}\"\n",
    "        \n",
    "    return text_blob\n",
    "\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "\n",
    "print(f\"üìù Combined text field created. Average length: {df['combined_text'].str.len().mean():.0f} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be513772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading model: all-MiniLM-L6-v2...\n",
      "üîÑ Pr√©paration : 19276 documents r√©partis en 603 batches.\n",
      "üöÄ D√©but/Reprise de la g√©n√©ration des embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 603/603 [03:01<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tous les batches sont calcul√©s et sauvegard√©s sur le disque.\n",
      "üì¶ Assemblage du fichier final...\n",
      "üéâ Termin√© ! Embeddings complets sauvegard√©s dans : output/articles_embeddings.npy\n",
      "Shape finale : (19276, 384)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "# --- CORRECTION ICI : On utilise tqdm standard pour √©viter l'erreur IProgress ---\n",
    "from tqdm import tqdm \n",
    "\n",
    "# --- Configuration ---\n",
    "CHECKPOINT_DIR = \"output/temp_embeddings\"  \n",
    "FINAL_EMBEDDING_FILE = \"output/articles_embeddings.npy\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Cr√©er le dossier de checkpoint s'il n'existe pas\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Initialisation ---\n",
    "print(f\"ü§ñ Loading model: {MODEL_NAME}...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "sentences = df['combined_text'].tolist()\n",
    "total_sentences = len(sentences)\n",
    "\n",
    "# Calcul du nombre total de batches\n",
    "num_batches = int(np.ceil(total_sentences / BATCH_SIZE))\n",
    "\n",
    "print(f\"üîÑ Pr√©paration : {total_sentences} documents r√©partis en {num_batches} batches.\")\n",
    "\n",
    "# --- 2. Boucle de g√©n√©ration avec reprise ---\n",
    "print(\"üöÄ D√©but/Reprise de la g√©n√©ration des embeddings...\")\n",
    "\n",
    "# On utilise tqdm() standard ici\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    # D√©finir les indices de d√©but et de fin pour ce lot\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, total_sentences)\n",
    "    \n",
    "    # Nom du fichier pour ce batch\n",
    "    batch_file = os.path.join(CHECKPOINT_DIR, f\"batch_{i:05d}.npy\")\n",
    "    \n",
    "    # V√âRIFICATION : Si le fichier existe d√©j√†, on le saute\n",
    "    if os.path.exists(batch_file):\n",
    "        continue\n",
    "        \n",
    "    # Sinon, on calcule\n",
    "    batch_sentences = sentences[start_idx:end_idx]\n",
    "    \n",
    "    # Encodage du lot\n",
    "    batch_embeddings = model.encode(batch_sentences, show_progress_bar=False)\n",
    "    \n",
    "    # Sauvegarde imm√©diate\n",
    "    np.save(batch_file, batch_embeddings)\n",
    "\n",
    "print(\"‚úÖ Tous les batches sont calcul√©s et sauvegard√©s sur le disque.\")\n",
    "\n",
    "# --- 3. Assemblage final ---\n",
    "print(\"üì¶ Assemblage du fichier final...\")\n",
    "\n",
    "batch_files = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"batch_*.npy\")))\n",
    "all_embeddings_list = [np.load(f) for f in batch_files]\n",
    "final_embeddings = np.vstack(all_embeddings_list)\n",
    "\n",
    "np.save(FINAL_EMBEDDING_FILE, final_embeddings)\n",
    "\n",
    "print(f\"üéâ Termin√© ! Embeddings complets sauvegard√©s dans : {FINAL_EMBEDDING_FILE}\")\n",
    "print(f\"Shape finale : {final_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a20c481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Output Saved Successfully:\n",
      "   1. Metadata:   output/articles_metadata.parquet\n",
      "   2. Embeddings: output/articles_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Prepare Metadata DataFrame\n",
    "# Select only fields useful for the UI/Filtering\n",
    "meta_cols = [\n",
    "    'doc_id', 'clean_title', 'clean_abstract', 'clean_authors', \n",
    "    'publication_year', 'doi', 'affiliation', 'ASJC_translation', 'cover_date'\n",
    "]\n",
    "\n",
    "# Ensure columns exist before selecting (handle potential missing cols in source)\n",
    "existing_cols = [c for c in meta_cols if c in df.columns]\n",
    "metadata_df = df[existing_cols].copy()\n",
    "\n",
    "# Rename for clarity in the UI app later\n",
    "metadata_df.rename(columns={\n",
    "    'clean_title': 'title',\n",
    "    'clean_abstract': 'abstract',\n",
    "    'clean_authors': 'authors',\n",
    "    'ASJC_translation': 'category'\n",
    "}, inplace=True)\n",
    "\n",
    "# 2. Save Files\n",
    "meta_path = os.path.join(OUTPUT_DIR, 'articles_metadata.parquet')\n",
    "embed_path = os.path.join(OUTPUT_DIR, 'articles_embeddings.npy')\n",
    "\n",
    "# Save Metadata (Critical step for the UI)\n",
    "metadata_df.to_parquet(meta_path, index=False)\n",
    "\n",
    "# Save Embeddings\n",
    "# NOTE: La cellule pr√©c√©dente l'a peut-√™tre d√©j√† fait, mais on s'assure ici \n",
    "# que tout est synchro. On utilise 'final_embeddings' qui est le r√©sultat de ton calcul.\n",
    "if 'final_embeddings' in locals():\n",
    "    np.save(embed_path, final_embeddings)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Attention : La variable 'final_embeddings' n'est pas en m√©moire.\")\n",
    "    print(\"Si tu as d√©j√† le fichier .npy g√©n√©r√© par la cellule pr√©c√©dente, c'est bon.\")\n",
    "\n",
    "print(\"üíæ Output Saved Successfully:\")\n",
    "print(f\"   1. Metadata:   {meta_path}\")\n",
    "print(f\"   2. Embeddings: {embed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cf50872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Diagnostic des colonnes...\n",
      "‚úÖ La colonne source 'authors' existe (Ex: Pratumsiri T.; Janpugdee P....)\n",
      "‚úÖ Colonne 'clean_authors' r√©g√©n√©r√©e.\n",
      "üíæ M√©tadonn√©es mises √† jour et sauvegard√©es dans : output/articles_metadata.parquet\n",
      "\n",
      "üîé Retest avec affichage des auteurs :\n",
      "\n",
      "üîé Query: 'generative ai energy efficiency'\n",
      "--------------------------------------------------\n",
      "Score: 0.5035 | Year: 2023\n",
      "Title: Future Distribution Power Flow Scenario Generation Method Using Generative Adversarial Network Considering Correlation Between DERs\n",
      "Authors: Ichinomiya H., Kawabe K., Chaitusaney S....\n",
      "\n",
      "Score: 0.4728 | Year: 2019\n",
      "Title: Generating images with desired properties using the discogan model enhanced with repeated property construction\n",
      "Authors: Angsarawanee T., Kijsirikul B....\n",
      "\n",
      "Score: 0.4373 | Year: 2023\n",
      "Title: A systematic and critical review on effective utilization of artificial intelligence for bio-diesel production techniques\n",
      "Authors: Ahmad J., Ngamcharussrivichai C., Awais M., Rashid U., Raza Naqvi S., Ali I....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELLULE DE R√âPARATION DES AUTEURS ---\n",
    "\n",
    "print(\"üîß Diagnostic des colonnes...\")\n",
    "\n",
    "# 1. V√©rifions si la colonne brute existe dans le DataFrame d'origine\n",
    "if 'authors' in df.columns:\n",
    "    print(f\"‚úÖ La colonne source 'authors' existe (Ex: {str(df['authors'].iloc[0])[:30]}...)\")\n",
    "else:\n",
    "    print(\"‚ùå La colonne 'authors' est introuvable dans df. V√©rifie le nom exact (maj/min).\")\n",
    "    # Tentative de retrouver la colonne (parfois 'Authors' avec majuscule)\n",
    "    possible_cols = [c for c in df.columns if 'author' in c.lower()]\n",
    "    print(f\"   Colonnes similaires trouv√©es : {possible_cols}\")\n",
    "\n",
    "# 2. On force la recr√©ation de la colonne propre\n",
    "def force_clean_authors(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return \"Unknown Author\"\n",
    "    return str(val).replace(';', ',')\n",
    "\n",
    "# On applique le nettoyage\n",
    "if 'authors' in df.columns:\n",
    "    df['clean_authors'] = df['authors'].apply(force_clean_authors)\n",
    "    print(\"‚úÖ Colonne 'clean_authors' r√©g√©n√©r√©e.\")\n",
    "\n",
    "# 3. On met √† jour metadata_df\n",
    "# On s'assure que 'clean_authors' est bien copi√©e\n",
    "metadata_df['authors'] = df['clean_authors']\n",
    "\n",
    "# 4. On sauvegarde √† nouveau pour √™tre s√ªr\n",
    "meta_path = os.path.join(OUTPUT_DIR, 'articles_metadata.parquet')\n",
    "metadata_df.to_parquet(meta_path, index=False)\n",
    "print(f\"üíæ M√©tadonn√©es mises √† jour et sauvegard√©es dans : {meta_path}\")\n",
    "\n",
    "# --- 5. RETEST IMM√âDIAT ---\n",
    "print(\"\\nüîé Retest avec affichage des auteurs :\")\n",
    "search_local_test(\"generative ai energy efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "834eb1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Query: 'generative ai energy efficiency'\n",
      "--------------------------------------------------\n",
      "Score: 0.5035 | Year: 2023\n",
      "Title: Future Distribution Power Flow Scenario Generation Method Using Generative Adversarial Network Considering Correlation Between DERs\n",
      "Authors: Unknown...\n",
      "\n",
      "Score: 0.4728 | Year: 2019\n",
      "Title: Generating images with desired properties using the discogan model enhanced with repeated property construction\n",
      "Authors: Unknown...\n",
      "\n",
      "Score: 0.4373 | Year: 2023\n",
      "Title: A systematic and critical review on effective utilization of artificial intelligence for bio-diesel production techniques\n",
      "Authors: Unknown...\n",
      "\n",
      "\n",
      "üîé Query: 'sustainable materials for construction'\n",
      "--------------------------------------------------\n",
      "Score: 0.6194 | Year: 2020\n",
      "Title: EMBODIED CARBON EMISSIONS OF CONSTRUCTION MATERIALS: A CASE STUDY OF BUILDINGS IN THAILAND\n",
      "Authors: Unknown...\n",
      "\n",
      "Score: 0.5860 | Year: 2018\n",
      "Title: Precast industry contributed toward green construction\n",
      "Authors: Unknown...\n",
      "\n",
      "Score: 0.5729 | Year: 2018\n",
      "Title: The influence of building envelop materials on its life cycle performance: A case study of educational building in Thailand\n",
      "Authors: Unknown...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_local_test(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Fonction simple pour tester la recherche dans le notebook.\n",
    "    Utilise 'final_embeddings' et g√®re les noms de colonnes de mani√®re robuste.\n",
    "    \"\"\"\n",
    "    # 1. Encode the query\n",
    "    # Le mod√®le est d√©j√† charg√© en m√©moire (model)\n",
    "    query_vec = model.encode([query])\n",
    "    \n",
    "    # 2. Compute Cosine Similarity\n",
    "    # CORRECTION ICI : On utilise 'final_embeddings' qui contient tous nos vecteurs assembl√©s\n",
    "    scores = np.dot(final_embeddings, query_vec.T).flatten()\n",
    "    \n",
    "    # 3. Get Top K indices\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # 4. Display results\n",
    "    print(f\"\\nüîé Query: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        row = metadata_df.iloc[idx]\n",
    "        score = scores[idx]\n",
    "        \n",
    "        # --- GESTION ROBUSTE DES COLONNES ---\n",
    "        # On essaie de r√©cup√©rer 'title', sinon on cherche 'clean_title', sinon 'chapter_title'\n",
    "        title = row.get('title', row.get('clean_title', row.get('chapter_title', 'Untitled')))\n",
    "        \n",
    "        # On essaie de r√©cup√©rer 'authors', sinon 'clean_authors', sinon 'Unknown'\n",
    "        authors = row.get('authors', row.get('clean_authors', 'Unknown'))\n",
    "        \n",
    "        # On g√®re l'ann√©e\n",
    "        year = row.get('publication_year', 'N/A')\n",
    "\n",
    "        print(f\"Score: {score:.4f} | Year: {year}\")\n",
    "        print(f\"Title: {title}\")\n",
    "        # On coupe la liste des auteurs si elle est trop longue pour l'affichage\n",
    "        print(f\"Authors: {str(authors)[:80]}...\") \n",
    "        print(\"\")\n",
    "\n",
    "# --- Run Test Cases ---\n",
    "# Note : Assure-toi que 'final_embeddings' et 'metadata_df' sont bien en m√©moire.\n",
    "if 'final_embeddings' in locals() and 'metadata_df' in locals():\n",
    "    search_local_test(\"generative ai energy efficiency\")\n",
    "    search_local_test(\"sustainable materials for construction\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Erreur : Les variables 'final_embeddings' ou 'metadata_df' ne sont pas d√©finies.\")\n",
    "    print(\"Assure-toi d'avoir ex√©cut√© les cellules pr√©c√©dentes (Assemblage et Sauvegarde).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a97526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
